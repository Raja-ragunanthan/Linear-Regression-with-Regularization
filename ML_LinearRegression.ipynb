{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Python Lab Group 10\n",
        "## Mohamamad Rafi Shaik\n",
        "## Rajaragunanthan Palanisamy"
      ],
      "metadata": {
        "id": "5TDOvtJpkClX"
      },
      "id": "5TDOvtJpkClX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a47366-87ef-4f40-ad62-6b70aaf45a9c",
      "metadata": {
        "id": "31a47366-87ef-4f40-ad62-6b70aaf45a9c"
      },
      "outputs": [],
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a24af4-87a2-40a7-a974-ec39d840e52d",
      "metadata": {
        "id": "b4a24af4-87a2-40a7-a974-ec39d840e52d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from random import sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ec6122-3776-48dc-aa0a-6d2a0be38b74",
      "metadata": {
        "id": "b1ec6122-3776-48dc-aa0a-6d2a0be38b74"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "file = files.upload()  #upload file into google colab session\n",
        "\n",
        "housing_df= pd.read_csv('housing.csv', header=None)\n",
        "\n",
        "concrete_df= pd.read_csv('concreteData.csv', header=None)\n",
        "\n",
        "yatch_df= pd.read_csv('yachtData.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing_df.head()"
      ],
      "metadata": {
        "id": "zN_kMHzHWkgm"
      },
      "id": "zN_kMHzHWkgm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concrete_df.head()"
      ],
      "metadata": {
        "id": "w-SqGKtVWrGw"
      },
      "id": "w-SqGKtVWrGw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yatch_df.head()"
      ],
      "metadata": {
        "id": "5AViCWv4WtQB"
      },
      "id": "5AViCWv4WtQB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps implemented\n",
        "1. Added regularization term using Norm 2\n",
        "  Normal equation (Closed form solution) \n",
        "Gradient Descent. \n",
        "Passed the regularization as a parameter to your class, so you could run your model with regularization term or without regularization term \n",
        "2. Added a function to calculate the Stochastic gradient descent.\n",
        "Be sure to pass the necessary parameters to your class so you could switch between gradient descent or stochastic gradient descent.\n",
        "Added a plotting function that will plot the error costs of the gradient descent.\n",
        "Be sure to plot the cost of your model during the training steps. \n",
        "3. Reported the RMSE and SSE over the test set for all three datasets. \n",
        "\n",
        "4. Provided our observations on hyper parameters like learning rate and regularization parameters on the learning."
      ],
      "metadata": {
        "id": "LA_2xoQljWf2"
      },
      "id": "LA_2xoQljWf2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1f786e-ebc8-4451-9eb0-7e0a67ca9daf",
      "metadata": {
        "id": "0c1f786e-ebc8-4451-9eb0-7e0a67ca9daf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self,\n",
        "                 X,\n",
        "                 y, learningRate,\n",
        "                 tolerance,\n",
        "                 maxIteration,\n",
        "                 gd=False,\n",
        "                 stochastic=False,\n",
        "                 reg=False,\n",
        "                 batch_size=100,\n",
        "                 alpha=1.0) -> None:\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tolerance = tolerance\n",
        "        self.learningRate = learningRate\n",
        "        self.maxIteration = maxIteration\n",
        "        self.gd = gd\n",
        "        self.stochastic = stochastic\n",
        "        self.reg = reg\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.errors = []\n",
        "\n",
        "    def splitTrainTest(self):\n",
        "        \"\"\"Splits the data into X_train, X_test, y_train, y_test\"\"\"\n",
        "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y,\n",
        "                                                            test_size=0.3,\n",
        "                                                            random_state=1)\n",
        "        return (X_train, X_test, y_train, y_test)\n",
        "\n",
        "    def add_X0(self, X):\n",
        "        \"\"\"Add new column at begining with all 1's for the intercept weight\"\"\"\n",
        "        return np.column_stack([np.ones([X.shape[0], 1]), X])\n",
        "\n",
        "    def normalize(self, X):\n",
        "        \"\"\"Z-Square standardization\"\"\"\n",
        "        mean = np.mean(X, 0)\n",
        "        std = np.std(X, 0)\n",
        "        X_norm = (X - mean) / std\n",
        "        X_norm = self.add_X0(X_norm)\n",
        "        return X_norm, mean, std\n",
        "\n",
        "    def normalizetestdata(self, X, meanTrain, stdTrain):\n",
        "        \"\"\"Normalize test data using parameters from training dataset\"\"\"\n",
        "        X_norm =  (X - meanTrain) / stdTrain\n",
        "        X_norm = self.add_X0(X_norm)\n",
        "        return X_norm\n",
        "\n",
        "    def l2_regularization(self, w):\n",
        "        \"\"\"Calculate regularization term.\"\"\"\n",
        "        return self.alpha * 0.5 * np.linalg.norm(w, 2)\n",
        "\n",
        "    def grad_reg_term(self, w):\n",
        "        \"\"\"Gradient penalty term\"\"\"\n",
        "        gradient_penalty = np.asarray(self.alpha) * w\n",
        "        # Insert 0 for bias term.\n",
        "        return np.insert(gradient_penalty, 0, 0, axis=0)\n",
        "\n",
        "    def checkMatrix(self, X):\n",
        "        \"\"\"Check for full rank of Matrix\"\"\"\n",
        "        X_rank = np.linalg.matrix_rank(X)\n",
        "\n",
        "        if X_rank == min(X.shape[0], X.shape[1]):\n",
        "            self.fullRank = True\n",
        "            print('data is full rank')\n",
        "        else:\n",
        "            self.fullRank = False\n",
        "            print('data is not full rank')\n",
        "\n",
        "    def checkInvertibility(self, X):\n",
        "        \"\"\"Check for invertibility of matrix\"\"\"\n",
        "        if X.shape[0] < X.shape[1]:\n",
        "            self.lowRank = True\n",
        "        else:\n",
        "            self.lowrank = False\n",
        "\n",
        "    def closedFormSolution(self, X, y):\n",
        "        \"\"\"Closed form solution\"\"\"\n",
        "        if self.reg:\n",
        "            w = np.linalg.inv(np.add(X.T.dot(X), np.identity(X.shape[1]))).dot(X.T).dot(y)\n",
        "            print('With Regularization term')\n",
        "        else:\n",
        "            w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "            print('Without Regularization term')\n",
        "        \n",
        "        return w\n",
        "\n",
        "    def sample_batch(self, X, y):\n",
        "        \"\"\"Random sample for stochastic regression\"\"\"\n",
        "        if not 0 < self.batch_size <= X.shape[0]:\n",
        "            raise ValueError(f\"'batch_size' must be greater than zero and less than or equal to the number of observations: {X.shape[0]}\")\n",
        "        s_range = sample(range(0,X.shape[0]), self.batch_size)\n",
        "        return X[s_range,], y[s_range,]\n",
        "\n",
        "    def gradientDescent(self, X, y):\n",
        "        \"\"\" Gradient Descent model\"\"\"\n",
        "        lastError = float('inf')\n",
        "        gradient_reg = 0\n",
        "        regularization = 0\n",
        "        for t in tqdm(range(self.maxIteration)):\n",
        "            if self.reg:\n",
        "                gradient_reg = self.grad_reg_term(self.w[1:])\n",
        "                regularization = self.l2_regularization(self.w[1:])\n",
        "\n",
        "            self.w = self.w - self.learningRate * (self.costDerivation(X, y) + gradient_reg)\n",
        "\n",
        "            # cur= self.sse(X_batch,y_batch) + regularization\n",
        "            cur = self.rmse(X, y) + regularization\n",
        "\n",
        "            diff = lastError - cur\n",
        "            lastError = cur\n",
        "            self.errors.append(cur)\n",
        "\n",
        "            if abs(diff) < self.tolerance:\n",
        "                print(\"The model stopped and no further improvement\")\n",
        "                break\n",
        "        #self.plot_cost(self.errors)\n",
        "\n",
        "\n",
        "    def stochasticGradientDescent(self, X, y):\n",
        "        \"\"\"Stochastic Gradient Descent model\"\"\"\n",
        "        lastError = float('inf')\n",
        "\n",
        "        gradient_reg = 0\n",
        "        regularization = 0\n",
        "        for t in tqdm(range(self.maxIteration)):\n",
        "            X_batch, y_batch = self.sample_batch(X, y)\n",
        "\n",
        "            if self.reg:\n",
        "                gradient_reg = self.grad_reg_term(self.w[1:])\n",
        "                regularization = self.l2_regularization(self.w[1:])\n",
        "\n",
        "            self.w = self.w - self.learningRate * (self.costDerivation(X_batch, y_batch) + gradient_reg)\n",
        "\n",
        "            # cur= self.sse(X_batch,y_batch) + regularization\n",
        "            cur = self.rmse(X_batch, y_batch) + regularization\n",
        "\n",
        "            diff = lastError - cur\n",
        "            lastError = cur\n",
        "            self.errors.append(cur)\n",
        "\n",
        "            if abs(diff) < self.tolerance:\n",
        "                print(\"The model stopped and no further improvement\")\n",
        "                break\n",
        "        #self.plot_cost(self.errors)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.w)\n",
        "\n",
        "    def sse(self, X, y):\n",
        "        \"\"\"Sum of Squared Errors\"\"\"\n",
        "        y_hat = self.predict(X)\n",
        "        return ((y_hat - y) ** 2).sum()\n",
        "\n",
        "    def rmse(self, X, y):\n",
        "        \"\"\"Root mean squares errors\"\"\"\n",
        "        n = X.shape[0]\n",
        "        return np.sqrt(self.sse(X, y) / n)\n",
        "\n",
        "    def costFunction(self, X, y):\n",
        "        return self.sse(X, y) / 2\n",
        "\n",
        "    def costDerivation(self, X, y):\n",
        "        y_hat = self.predict(X)\n",
        "        return (y_hat - y).dot(X)\n",
        "\n",
        "    def plot_cost(self, cost_sequence):\n",
        "        # Data for plotting\n",
        "        s = np.array(cost_sequence)\n",
        "        t = np.arange(s.size)\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(t, s)\n",
        "\n",
        "        ax.set(xlabel='iterations', ylabel='cost',\n",
        "               title='cost trend')\n",
        "        ax.grid()\n",
        "\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, shadow=True)\n",
        "        plt.show()\n",
        "\n",
        "    def fit(self):\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = self.splitTrainTest()\n",
        "\n",
        "        # normalize the data\n",
        "        self.X_train, meanTrain, stdTrain = self.normalize(self.X_train)\n",
        "        self.X_test = self.normalizetestdata(self.X_test, meanTrain, stdTrain)\n",
        "\n",
        "        self.checkInvertibility(self.X_train)\n",
        "        self.checkMatrix(self.X_train)\n",
        "\n",
        "        if self.fullRank and not self.lowrank and not self.gd:\n",
        "            print(\"Solving using Normal equation - closed form solution\")\n",
        "            self.w = self.closedFormSolution(self.X_train, self.y_train)\n",
        "\n",
        "        elif self.gd and not self.stochastic:\n",
        "            print(\"Solving using gradientDescent\")\n",
        "            self.w = np.ones(self.X_train.shape[1], dtype=np.float64) * 0\n",
        "            self.gradientDescent(self.X_train, self.y_train)\n",
        "\n",
        "        elif self.gd and self.stochastic:\n",
        "            print(\"Solving using stochasticGradientDescent\")\n",
        "            self.w = np.ones(self.X_train.shape[1], dtype=np.float64) * 0\n",
        "            self.stochasticGradientDescent(self.X_train, self.y_train)\n",
        "\n",
        "        #print(self.w)\n",
        "\n",
        "    def predict_test(self):\n",
        "        \n",
        "        y_hat_test = self.predict(self.X_test)\n",
        "        rmse_test = self.rmse(self.X_test, self.y_test)\n",
        "        sse_test = self.sse(self.X_test, self.y_test)\n",
        "        return y_hat_test, rmse_test, sse_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\n",
        "    {\n",
        "        'name': 'housing',\n",
        "        'kwargs': {\n",
        "            'X': housing_df.values[:, 0:-1],\n",
        "            'y': housing_df.values[:, -1],\n",
        "            'learningRate': 0.0004,\n",
        "            'tolerance': 0.005,\n",
        "            'maxIteration': 50000,\n",
        "            'gd': True,\n",
        "            'stochastic': True,\n",
        "            'reg': True,\n",
        "            'batch_size': 100,\n",
        "            'alpha': 1.0\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'concrete',\n",
        "        'kwargs': {\n",
        "            'X': concrete_df.values[:, 0:-1],\n",
        "            'y': concrete_df.values[:, -1],\n",
        "            'learningRate': 0.0007,\n",
        "            'tolerance': 0.0001,\n",
        "            'maxIteration': 50000,\n",
        "            'gd': True,\n",
        "            'stochastic': True,\n",
        "            'reg': True,\n",
        "            'batch_size': 100,\n",
        "            'alpha': 1.0\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'yatch',\n",
        "        'kwargs': {\n",
        "            'X': yatch_df.values[:, 0:-1],\n",
        "            'y': yatch_df.values[:, -1],\n",
        "            'learningRate': 0.001,\n",
        "            'tolerance': 0.001,\n",
        "            'maxIteration': 50000,\n",
        "            'gd': True,\n",
        "            'stochastic': True,\n",
        "            'reg': True,\n",
        "            'batch_size': 100,\n",
        "            'alpha': 1.0\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "for d in datasets:\n",
        "    print(f\"Running Model on dataset: {d['name']}\")\n",
        "    \n",
        "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
        "    for i in [0,1]:\n",
        "      for j in [0,1]:\n",
        "        d['kwargs']['reg'] = True if i==1 else False\n",
        "        d['kwargs']['stochastic'] = True if i==1 else False\n",
        "        model = LinearRegression(**d['kwargs'])\n",
        "        model.fit()\n",
        "        result, rmse_test, sse_test = model.predict_test()\n",
        "        axs[i, j].plot(model.errors)\n",
        "        axs[i, j].set_title(f\"\"\"Cost trend for {d['name']}: {'with regularization' if i==1 else ''} using {'Stochastic' if j==1 else ''} Gradient Descent \n",
        "        RMSE on test: {rmse_test}, SSE on test: {sse_test}\"\"\")\n",
        "        \n",
        "        print(rmse_test, sse_test)\n",
        "\n",
        "    for ax in axs.flat:\n",
        "        ax.grid()\n",
        "        ax.set(xlabel='iterations', ylabel='cost')\n",
        "        \n"
      ],
      "metadata": {
        "id": "w17ZRYn5XuFv"
      },
      "id": "w17ZRYn5XuFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations\n",
        "\n",
        " **1. Stochastic model helps reach minimum values fast because of using only subset of data drawn randomly for training.**\n",
        "\n",
        " **2. Adding regularization helps skip local minimums by adding a small bias and in turn reduces variance to a considerable amount**\n",
        " \n",
        " **3. Increase in the learning rate makes the model to reach the minimum faster while making it close to 1 makes the model to bounce back without touching the global minima.**\n",
        "\n",
        " **4. Tolerance is used to not run the model once we got acceptable error difference or there is nothing more to learn**\n",
        "\n",
        " **5. MaxIterations is to avoid run model forever.**"
      ],
      "metadata": {
        "id": "R3MIVEfcXQt5"
      },
      "id": "R3MIVEfcXQt5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "ML - LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}